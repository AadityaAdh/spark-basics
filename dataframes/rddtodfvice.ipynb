{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b847b5-5338-4439-bc0e-24d0c3e81264",
   "metadata": {},
   "source": [
    "Hereâ€™s how to convert between **DataFrame** and **RDD** in PySpark:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **DataFrame â†’ RDD**\n",
    "\n",
    "Each DataFrame has an `.rdd` attribute returning an `RDD[Row]`.\n",
    "\n",
    "```python\n",
    "rdd = df.rdd\n",
    "```\n",
    "\n",
    "* This gives you an RDD of `Row` objects.\n",
    "* If youâ€™d rather have tuples or lists:\n",
    "\n",
    "```python\n",
    "rdd_tuples = df.rdd.map(tuple)\n",
    "rdd_lists = df.rdd.map(list)\n",
    "```\n",
    "\n",
    "([stackoverflow.com][1])\n",
    "\n",
    "Use RDD methods like `.map()`, `.filter()`, `.flatMap()`, etc.\n",
    "Example:\n",
    "\n",
    "```python\n",
    "names_rdd = df.rdd.map(lambda row: row['Name'])\n",
    "print(names_rdd.collect())\n",
    "```\n",
    "\n",
    "([techbrothersit.com][2])\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **RDD â†’ DataFrame**\n",
    "\n",
    "You can go back in two main ways:\n",
    "\n",
    "### A) Using RDD's `.toDF()` (requires implicit column names)\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30)])\n",
    "df2 = rdd.toDF([\"Name\", \"Age\"])\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "This works if the RDD is of tuples/lists and you provide the column names.\n",
    "\n",
    "\n",
    "### B) Using `SparkSession.createDataFrame()`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([Row(Name=\"Alice\", Age=25), Row(Name=\"Bob\", Age=30)])\n",
    "df3 = spark.createDataFrame(rdd)  # Schema inferred\n",
    "df3.show()\n",
    "```\n",
    "\n",
    "Or with tuples and schema list:\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30)])\n",
    "schema = [\"Name\", \"Age\"]\n",
    "df4 = spark.createDataFrame(rdd, schema)\n",
    "df4.show()\n",
    "```\n",
    "\n",
    "([geeksforgeeks.org][3])\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§­ When to Convert?\n",
    "\n",
    "* Use DataFrames for structured, SQL-like operations (optimized via Catalyst)\n",
    "* Use RDDs for custom, low-level transformations or legacy APIs\n",
    "  ([linkedin.com][4])\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick Summary Table\n",
    "\n",
    "| From â†’ To | Method                               | Notes                         |\n",
    "| --------- | ------------------------------------ | ----------------------------- |\n",
    "| DF â†’ RDD  | `df.rdd` â†’ `.map(tuple)` optional    | Rows of type `Row` by default |\n",
    "| RDD â†’ DF  | `rdd.toDF([...])`                    | Easy when you have columns    |\n",
    "|           | `spark.createDataFrame(rdd, schema)` | Full control with schema      |\n",
    "\n",
    "---\n",
    "\n",
    "Need a concrete example or help with a specific useâ€‘case? Just ask!\n",
    "\n",
    "[1]: https://stackoverflow.com/questions/29000514/how-to-convert-a-dataframe-back-to-normal-rdd-in-pyspark?utm_source=chatgpt.com \"How to convert a DataFrame back to normal RDD in pyspark?\"\n",
    "[2]: https://www.techbrothersit.com/2025/05/how-to-convert-pyspark-dataframe-to-rdd.html?utm_source=chatgpt.com \"How to Convert PySpark DataFrame to RDD Using .rdd\"\n",
    "[3]: https://www.geeksforgeeks.org/python/convert-pyspark-rdd-to-dataframe/?utm_source=chatgpt.com \"Convert PySpark RDD to DataFrame - GeeksforGeeks\"\n",
    "[4]: https://www.linkedin.com/pulse/understanding-when-convert-spark-dataframes-rdds-practical-khaled-aoiif?utm_source=chatgpt.com \"Understanding When to Convert Spark DataFrames to RDDs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40ae49-0ca4-4f26-b4d0-23eaeec79d48",
   "metadata": {},
   "source": [
    "# yellai yesari bhujam df -> rdd\n",
    "\n",
    "malai tyo bastraction functionality tyo select where testo aaile garnu xaina\n",
    "\n",
    "malai map haru testo gernu xa \n",
    "\n",
    "like if you want to map each element you will need to do this\n",
    "\n",
    "note garam \n",
    "\n",
    "df ma convert vai sakae paxi\n",
    "\n",
    "pratyek element in that parition is row object hunxa hai\n",
    "\n",
    "eg \n",
    "like [\"abc\",\"fa\",\"dadaf\"]\n",
    "\n",
    "yesto ma abc vanae ko yeuta element of partition hunthyo vanae\n",
    "\n",
    "\n",
    "aaba partition ko yeuta element vanae ko row object hunxa\n",
    "\n",
    "\n",
    "like row(....attributes) vanae ko yeuta single element of partition vayo\n",
    "\n",
    "aaba map garda pani yeslai map huni gari /yo row object lai map huni gari map garni\n",
    "\n",
    "\n",
    "so simply within each partition every element of partition is row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf25b5ea-2161-4c5a-af0a-9c1ac8390528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 08:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalone\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893aac71-0cee-4cc5-8a22-292cd9919daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"Name\": \"Alice\", \"Age\": 25}, {\"Name\": \"Bob\", \"Age\": 30}]\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90b9ec4-3348-4063-a457-b3acbdd6beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa14d992-0ca7-4f10-9b9a-8b6fe0c8faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Age=25, Name='Alice'), Row(Age=30, Name='Bob')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cb35f-8fdf-4b46-bbaa-1e5dd4db4122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b36307-ca2b-488e-bab8-26355f4de3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now aaba map garda pani yeuta element chai row ho vanae ra garni\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928b282-9028-414a-a4d7-67338a0c302f",
   "metadata": {},
   "source": [
    "You can access the values in a Spark `Row` object in two main ways:\n",
    "\n",
    "### 1. Access by column name (recommended)\n",
    "\n",
    "```python\n",
    "for row in df.collect():\n",
    "    print(row.name, row.roll)\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```python\n",
    "print(row[\"name\"], row[\"roll\"])\n",
    "```\n",
    "\n",
    "### 2. Access by position (index)\n",
    "\n",
    "```python\n",
    "for row in df.collect():\n",
    "    print(row[0], row[1])  # 0 for first column, 1 for second column\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"roll\"])\n",
    "\n",
    "rows = df.collect()\n",
    "for row in rows:\n",
    "    print(row.name, row.roll)       # by column name\n",
    "    print(row[\"name\"], row[\"roll\"]) # by key\n",
    "    print(row[0], row[1])            # by index\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a snippet for accessing rows without collecting all at once (e.g., with `df.take()` or `df.foreach()`)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79adba-6e2a-49e2-b87e-bf1154033440",
   "metadata": {},
   "source": [
    "# vizualize this as this\n",
    "\n",
    "when i do rdd.todf\n",
    "\n",
    "it wll create new rdd with each element as row object \n",
    "\n",
    "also this is df so abstraction on that rdd\n",
    "\n",
    "and the original rdd remains intact \n",
    "\n",
    "\n",
    "# this is not verified though i am just giving this to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefaebe-27ce-427a-a99a-8a0852c4fcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
