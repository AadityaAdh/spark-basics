{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c17e399-23a0-4f22-b9f5-e2b137407725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 10:05:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalo\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef3c53e-bcf4-4a7d-9a5c-90f7ec3be05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead5bee7-ab40-4ca9-a2cb-39d32c06d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "# Initialize our data\n",
    "data2 = [(\"Pulkit\", 12, \"CS32\", 82, \"Programming\"),\n",
    "         (\"Ritika\", 20, \"CS32\", 94, \"Writing\"),\n",
    "         (\"Atirikt\", 4, \"BB21\", 78, None),\n",
    "         (\"Reshav\", 18, None, 56, None)\n",
    "         ]\n",
    "\n",
    "# Start spark session\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Roll Number\", IntegerType(), True),\n",
    "    StructField(\"Class ID\", StringType(), True),\n",
    "    StructField(\"Marks\", IntegerType(), True),\n",
    "    StructField(\"Extracurricular\", StringType(), True)\n",
    "])\n",
    "\n",
    "# read the dataframe\n",
    "df = spark.createDataFrame(data=data2, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715a85a2-3f1b-45ec-96a4-90c68410dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----+---------------+\n",
      "|   Name|Roll Number|Class ID|Marks|Extracurricular|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "| Pulkit|         12|    CS32|   82|    Programming|\n",
      "| Ritika|         20|    CS32|   94|        Writing|\n",
      "|Atirikt|          4|    BB21|   78|           NULL|\n",
      "| Reshav|         18|    NULL|   56|           NULL|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924dc4c-c0fb-4013-be3d-53e266f262fb",
   "metadata": {},
   "source": [
    "# 1: Select single or multiple columns\n",
    "\n",
    "We can select single or multiple columns using the select() function by specifying the particular column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1113c7-9b5d-45ac-b1d0-46e3a671e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.select(\"Name\", \"Marks\",\"Extracurricular\")\n",
    "df2=df.select(\"Name\", \"Marks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaacbb68-65cb-4e4e-9e6c-34a9b4b0decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------------+\n",
      "|   Name|Marks|Extracurricular|\n",
      "+-------+-----+---------------+\n",
      "| Pulkit|   82|    Programming|\n",
      "| Ritika|   94|        Writing|\n",
      "|Atirikt|   78|           NULL|\n",
      "| Reshav|   56|           NULL|\n",
      "+-------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56df860-a1bb-49e4-abf1-33622960f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Marks|\n",
      "+-------+-----+\n",
      "| Pulkit|   82|\n",
      "| Ritika|   94|\n",
      "|Atirikt|   78|\n",
      "| Reshav|   56|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049d666a-6b66-4751-b7de-71b03698b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just visualize it as new rdd and dataframe built on top of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cfb58f-3d46-4ecf-bd71-3fbe32eae9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Roll Number', 'Class ID', 'Marks', 'Extracurricular']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "# this is neither transformation nor action\n",
    "# its just like a function that is executed in this driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3d72d2-0f91-4663-9d47-6b6bf9c5e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Marks|\n",
      "+-------+-----+\n",
      "| Pulkit|   82|\n",
      "| Ritika|   94|\n",
      "|Atirikt|   78|\n",
      "| Reshav|   56|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now you can use this approach to select as well\n",
    "df.select([\"Name\",\"Marks\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65180c18-9bf8-4163-a301-e7ca7c925ec5",
   "metadata": {},
   "source": [
    "# 2.Select columns using indexing\n",
    "\n",
    "Indexing provides an easy way of accessing columns inside a dataframe. Indexing starts from 0 and has total n-1 numbers representing each column with 0 as first and n-1 as last nth column. We can use df.columns to access all the columns and use indexing to pass in the required columns inside a select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b8b794-d077-477c-8fe4-6a0096346505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   Name|Roll Number|\n",
      "+-------+-----------+\n",
      "| Pulkit|         12|\n",
      "| Ritika|         20|\n",
      "|Atirikt|          4|\n",
      "| Reshav|         18|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# so you can do \n",
    "df.select(df.columns[:2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d9410-db5a-47d5-bee1-73179461e851",
   "metadata": {},
   "source": [
    "# 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6924d078-dde6-46fa-b788-2fe57d89ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Marks|\n",
      "+-------+-----+\n",
      "| Pulkit|   82|\n",
      "| Ritika|   94|\n",
      "|Atirikt|   78|\n",
      "| Reshav|   56|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Name,df.Marks).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff661a0-9972-44b3-b346-fa297e10fb67",
   "metadata": {},
   "source": [
    "# 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc005f40-d1e8-4ed3-b182-9b38ba0c7378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Marks|\n",
      "+-------+-----+\n",
      "| Pulkit|   82|\n",
      "| Ritika|   94|\n",
      "|Atirikt|   78|\n",
      "| Reshav|   56|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Name\"],df[\"Marks\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056cddc-7b17-4da0-ba36-b317aa5e98e1",
   "metadata": {},
   "source": [
    "# 5 We can use col() function from pyspark.sql.functions module to specify the particular columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25a8cb8-8015-462e-9e46-a3966011bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Marks|\n",
      "+-------+-----+\n",
      "| Pulkit|   82|\n",
      "| Ritika|   94|\n",
      "|Atirikt|   78|\n",
      "| Reshav|   56|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"Name\"),col(\"Marks\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0310e-0c3f-4058-8f2c-18a70f5fb5eb",
   "metadata": {},
   "source": [
    "The `col()` function in PySpark is used to **refer to a column** in a DataFrame in a way that allows you to apply transformations or build expressions.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Why use `col()`?\n",
    "\n",
    "* It’s required when using **column expressions** in transformations like:\n",
    "\n",
    "  * `select()`, `filter()`, `withColumn()`, etc.\n",
    "* It gives **more flexibility** and is cleaner than using string column names directly.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Basic Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"name\"), col(\"age\") + 1).show()\n",
    "```\n",
    "\n",
    "* `col(\"age\") + 1` adds 1 to each value in the `age` column.\n",
    "* You can't write `\"age\" + 1` directly — that would throw an error.\n",
    "\n",
    "---\n",
    "\n",
    "### 🆚 Without `col()` — works in some cases, but limited\n",
    "\n",
    "```python\n",
    "df.select(\"name\", \"age\").show()       # okay\n",
    "df.select(\"age\" + 1).show()           # ❌ error\n",
    "```\n",
    "\n",
    "With `col()` you can write expressions:\n",
    "\n",
    "```python\n",
    "df.select(col(\"age\") + 1).show()      # ✅ works\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Used with other functions:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import upper, when\n",
    "\n",
    "df.select(\n",
    "    upper(col(\"name\")).alias(\"NAME_UPPER\"),\n",
    "    when(col(\"age\") >= 18, \"Adult\").otherwise(\"Minor\").alias(\"status\")\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary\n",
    "\n",
    "| Function             | Description                                  |\n",
    "| -------------------- | -------------------------------------------- |\n",
    "| `col(\"column_name\")` | Refers to a DataFrame column for expressions |\n",
    "| Used in              | `select()`, `filter()`, `withColumn()`, etc. |\n",
    "\n",
    "It’s a **standard way to refer to and manipulate columns** safely and expressively in Spark DataFrame operations.\n",
    "\n",
    "Let me know if you want comparisons with `expr()` or `df[\"column\"]` too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f635383-ca01-4fdc-9b96-3baf76375b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb67268-e297-4023-9c04-30bbfc9bc394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
