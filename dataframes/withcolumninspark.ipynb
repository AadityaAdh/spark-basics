{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cdfa3c-7eda-49af-af73-f157d172e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 11:23:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalo\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10dbeea-2fc4-4cb8-ba9a-4e9605b8131d",
   "metadata": {},
   "source": [
    "### âœ… `withColumn()` in Spark\n",
    "\n",
    "`withColumn()` is a method in PySpark DataFrame used to:\n",
    "\n",
    "* **Add a new column**, or\n",
    "* **Replace an existing column**\n",
    "  by applying an **expression or transformation**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Syntax:\n",
    "\n",
    "```python\n",
    "df.withColumn(\"new_column_name\", expression)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Examples\n",
    "\n",
    "#### 1. **Add a new column by transforming an existing one**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn(\"age_plus_1\", col(\"age\") + 1).show()\n",
    "```\n",
    "\n",
    "#### 2. **Replace an existing column**\n",
    "\n",
    "If the column already exists, it will be **overwritten**:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"age\", col(\"age\") + 10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Chaining `withColumn()` multiple times\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"age_plus_5\", col(\"age\") + 5) \\\n",
    "       .withColumn(\"is_adult\", col(\"age\") >= 18)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Common mistakes\n",
    "\n",
    "| Mistake                   | Fix                                    |\n",
    "| ------------------------- | -------------------------------------- |\n",
    "| Using `df.col(\"age\")`     | Use `col(\"age\")` or `df[\"age\"]`        |\n",
    "| Expecting in-place update | `withColumn()` returns a new DataFrame |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Use Cases\n",
    "\n",
    "* Creating flags or categories with `when()`, `otherwise()`\n",
    "* Data normalization/scaling\n",
    "* Type conversion\n",
    "* Creating columns from other columns\n",
    "\n",
    "Example with `when()`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\"status\", when(col(\"age\") >= 18, \"Adult\").otherwise(\"Minor\")).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like examples with `UDF`, conditionals, or string operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54086d6-27e2-46be-9e36-ba51a635f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "# Initialize our data\n",
    "data2 = [(\"Pulkit\", 12, \"CS32\", 82, \"Programming\"),\n",
    "         (\"Ritika\", 20, \"CS32\", 94, \"Writing\"),\n",
    "         (\"Atirikt\", 4, \"BB21\", 78, None),\n",
    "         (\"Reshav\", 18, None, 56, None)\n",
    "         ]\n",
    "\n",
    "# Start spark session\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Roll Number\", IntegerType(), True),\n",
    "    StructField(\"Class ID\", StringType(), True),\n",
    "    StructField(\"Marks\", IntegerType(), True),\n",
    "    StructField(\"Extracurricular\", StringType(), True)\n",
    "])\n",
    "\n",
    "# read the dataframe\n",
    "df = spark.createDataFrame(data=data2, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42e4aa4-9791-48b1-84ce-42b8c2db4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----+---------------+\n",
      "|   Name|Roll Number|Class ID|Marks|Extracurricular|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "| Pulkit|         12|    CS32|   82|    Programming|\n",
      "| Ritika|         20|    CS32|   94|        Writing|\n",
      "|Atirikt|          4|    BB21|   78|           NULL|\n",
      "| Reshav|         18|    NULL|   56|           NULL|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856a7b6-1ebe-4464-9a3e-f5403b4d00ad",
   "metadata": {},
   "source": [
    "# with column le baki sabbai column jasta ko testai\n",
    "\n",
    "but yes vitra define vako column herxa yedi xa vanae teslai modify natra naya column add \n",
    "\n",
    "note naya dtataframe retun hunxa hai \n",
    "\n",
    "tesai ma garu pare df=df.with.... garni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbe15c6-1488-44f0-b0e3-1ebf149f0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "df1 = df.withColumn(\"Class ID\", upper(col(\"Name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a710615-2ccf-4480-bfc1-36663730c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/24 11:29:19 ERROR TaskSchedulerImpl: Lost executor 1 on 172.20.0.3: Command exited with code 137\n",
      "25/06/24 11:29:19 WARN TaskSetManager: Lost task 3.0 in stage 4.0 (TID 22) (172.20.0.3 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/06/24 11:29:21 ERROR TaskSchedulerImpl: Lost executor 2 on 172.20.0.2: Command exited with code 137\n",
      "25/06/24 11:29:21 WARN TaskSetManager: Lost task 3.1 in stage 4.0 (TID 23) (172.20.0.2 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----+---------------+\n",
      "|   Name|Roll Number|Class ID|Marks|Extracurricular|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "| Pulkit|         12|  PULKIT|   82|    Programming|\n",
      "| Ritika|         20|  RITIKA|   94|        Writing|\n",
      "|Atirikt|          4| ATIRIKT|   78|           NULL|\n",
      "| Reshav|         18|  RESHAV|   56|           NULL|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/24 11:29:50 WARN TransportChannelHandler: Exception in connection from /172.20.0.4:37288\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:394)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:426)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:255)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:356)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:796)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/24 11:29:51 ERROR TaskSchedulerImpl: Lost executor 0 on 172.20.0.4: Command exited with code 137\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef992a4f-c3cd-4281-b141-c69bdc2fb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yesma when tyo case wala chai garna mildo raixa like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7457f8-b8cc-4c95-819b-7c76fafa530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----+---------------+------+\n",
      "|   Name|Roll Number|Class ID|Marks|Extracurricular|status|\n",
      "+-------+-----------+--------+-----+---------------+------+\n",
      "| Pulkit|         12|    CS32|   82|    Programming|  Good|\n",
      "| Ritika|         20|    CS32|   94|        Writing|  Good|\n",
      "|Atirikt|          4|    BB21|   78|           NULL|   Bad|\n",
      "| Reshav|         18|    NULL|   56|           NULL|   Bad|\n",
      "+-------+-----------+--------+-----+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df.withColumn(\"status\", when(col(\"Marks\") >= 80, \"Good\").otherwise(\"Bad\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "813a2421-59fe-47b0-98f0-2d011b1beab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but aaba complex logic lekhu pare udf lekhnu parxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796eaeb2-68fe-4a53-892d-8b32ccb42824",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8db2c-dfa5-4b2e-aab8-53fe9bdd4621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
