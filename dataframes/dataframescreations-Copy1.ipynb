{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa0f4a5-b0f4-4acd-8ef6-385973a95cbd",
   "metadata": {},
   "source": [
    "# aaba dataframe lai higher level abstraction built on top of rdd vanae ra bhujam\n",
    "\n",
    "rdd mai naya naya function lyako vanae ra bhujam\n",
    "\n",
    "rdd batai kei garnu paro vanae kati lamo lekhnu parxa paxi bhujna ni garo hunxa\n",
    "\n",
    "but yesko tyo function bata bhujna ni sajilo sano sano line lekhae ni pugxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206fd667-34b7-40cb-91d1-2b92fc9aa594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame ma query optimizer hunxa tei vayae ra fast like filter paila launu parxa testo aafai gardinxa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac301e48-5f7a-4d70-8e60-86d93f95246c",
   "metadata": {},
   "source": [
    "# also major benefit of dataframe over rdd in oython\n",
    "\n",
    "python ma rdd transformation garda lambda function or python function lekhinxa\n",
    "\n",
    "spark ta jvm -java ho \n",
    "\n",
    "so tyo python function execute garna python jvm banauxa\n",
    "\n",
    "aani tyo function tya ship hunxa - executor mai hai\n",
    "\n",
    "so slow hunxa aaru vanda\n",
    "\n",
    "but dataframe ma yesto function haru pani use hunna \n",
    "\n",
    "so simple explanation jasto matrai ho ni ta yo vanae ko ta so faster while using python -use infoQ video of spark streaming brian copler video -i used that reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db88fb6-eb37-4f7e-9b1a-309013e21816",
   "metadata": {},
   "source": [
    "In **PySpark** (the Python API for Apache Spark), you can create **DataFrames** in several ways. Here's a quick guide to common methods:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. **From a list of tuples**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreateDF\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "|Alice| 25|\n",
    "|  Bob| 30|\n",
    "|Cathy| 28|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. **From a list of dicts**\n",
    "\n",
    "```python\n",
    "data = [{\"Name\": \"Alice\", \"Age\": 25}, {\"Name\": \"Bob\", \"Age\": 30}]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. **From an RDD**\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30)])\n",
    "df = spark.createDataFrame(rdd, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. **From a CSV / JSON / Parquet file**\n",
    "\n",
    "```python\n",
    "# CSV\n",
    "df_csv = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# JSON\n",
    "df_json = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Parquet\n",
    "df_parquet = spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "df_csv.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. **With explicit schema**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see how to write a DataFrame back to disk or perform operations on it (like filtering, joining, etc.)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897f155d-be79-4b31-a77a-f57a35fa4e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 07:49:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalone\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddccf448-1868-467f-bf8a-9d6463aef82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age| Name|\n",
      "+---+-----+\n",
      "| 25|Alice|\n",
      "| 30|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = [{\"Name\": \"Alice\", \"Age\": 25}, {\"Name\": \"Bob\", \"Age\": 30}]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c577bd0-84b2-47fd-92ce-90ad920ba02e",
   "metadata": {},
   "source": [
    "The header and schema are separate things.\n",
    "\n",
    "Header:\n",
    "\n",
    "If the csv file have a header (column names in the first row) then set header=true. This will use the first row in the csv file as the dataframe's column names. Setting header=false (default option) will result in a dataframe with default column names: _c0, _c1, _c2, etc.\n",
    "\n",
    "Setting this to true or false should be based on your input file.\n",
    "\n",
    "Schema:\n",
    "\n",
    "The schema refered to here are the column types. A column can be of type String, Double, Long, etc. Using inferSchema=false (default option) will give a dataframe where all columns are strings (StringType). Depending on what you want to do, strings may not work. For example, if you want to add numbers from different columns, then those columns should be of some numeric type (strings won't work).\n",
    "\n",
    "By setting inferSchema=true, Spark will automatically go through the csv file and infer the schema of each column. This requires an extra pass over the file which will result in reading a file with inferSchema set to true being slower. But in return the dataframe will most likely have a correct schema given its input.\n",
    "\n",
    "As an alternative to reading a csv with inferSchema you can provide the schema while reading. This have the advantage of being faster than inferring the schema while giving a dataframe with the correct column types. In addition, for csv files without a header row, column names can be given automatically. To provde schema see e.g.: Provide schema while reading csv file as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d6556-caa4-4471-b0dc-5dd4beff4b47",
   "metadata": {},
   "source": [
    "Great question!\n",
    "\n",
    "### üîπ `header=True` (or `header=\"true\"`) is used when **reading CSV files** in PySpark to tell Spark that the **first line of the file contains column names**, not data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Example:\n",
    "\n",
    "Say you have a CSV file `people.csv`:\n",
    "\n",
    "```csv\n",
    "Name,Age\n",
    "Alice,25\n",
    "Bob,30\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ With `header=True`:\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "|Alice| 25|\n",
    "|  Bob| 30|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "* Spark **uses the first row** (`Name,Age`) as column names.\n",
    "* With `inferSchema=True`, it guesses `Age` is an integer, not a string.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è Without `header=True`:\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"people.csv\", header=False)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "+-------+-----+\n",
    "|   _c0 | _c1 |\n",
    "+-------+-----+\n",
    "|  Name | Age |\n",
    "| Alice|  25  |\n",
    "|  Bob |  30  |\n",
    "+-------+-----+\n",
    "```\n",
    "\n",
    "* Spark treats **all rows as data**, including the header.\n",
    "* Column names default to `_c0`, `_c1`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary:\n",
    "\n",
    "| Option             | Description                                    |\n",
    "| ------------------ | ---------------------------------------------- |\n",
    "| `header=True`      | First row is used as column names              |\n",
    "| `header=False`     | First row is treated as data                   |\n",
    "| `inferSchema=True` | Automatically detects column types (int, str‚Ä¶) |\n",
    "\n",
    "Let me know if you want to write CSVs with headers too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fce1e4-e4d7-4753-8eee-9eb58f250b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2d7d4-5570-4fb3-a4e1-814ad1c3a657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
