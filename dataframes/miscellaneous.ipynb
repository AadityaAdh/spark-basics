{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23eecc5c-9ed6-43af-924b-4510a080dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 10:46:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalo\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7932e7-5856-4895-97c2-021727fcc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "# Initialize our data\n",
    "data2 = [(\"Pulkit\", 12, \"CS32\", 82, \"Programming\"),\n",
    "         (\"Ritika\", 20, \"CS32\", 94, \"Writing\"),\n",
    "         (\"Atirikt\", 4, \"BB21\", 78, None),\n",
    "         (\"Reshav\", 18, None, 56, None)\n",
    "         ]\n",
    "\n",
    "# Start spark session\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Roll Number\", IntegerType(), True),\n",
    "    StructField(\"Class ID\", StringType(), True),\n",
    "    StructField(\"Marks\", IntegerType(), True),\n",
    "    StructField(\"Extracurricular\", StringType(), True)\n",
    "])\n",
    "\n",
    "# read the dataframe\n",
    "df = spark.createDataFrame(data=data2, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b244e6-7ca8-4e13-a424-8007f03b8dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----+---------------+\n",
      "|   Name|Roll Number|Class ID|Marks|Extracurricular|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "| Pulkit|         12|    CS32|   82|    Programming|\n",
      "| Ritika|         20|    CS32|   94|        Writing|\n",
      "|Atirikt|          4|    BB21|   78|           NULL|\n",
      "| Reshav|         18|    NULL|   56|           NULL|\n",
      "+-------+-----------+--------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beb2e69-0ef2-4ef7-8e08-946d38ad1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0845a56-facf-46e6-a996-d903f4352082",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.groupBy(\"Name\").agg({\"Marks\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561d4c58-882f-4c0b-a62a-4309f75be325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   Name|sum(Marks)|\n",
      "+-------+----------+\n",
      "| Ritika|        94|\n",
      "| Reshav|        56|\n",
      "| Pulkit|        82|\n",
      "|Atirikt|        78|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beca13f-1cba-4407-b4e7-b608ff8277f7",
   "metadata": {},
   "source": [
    "You can apply a wide variety of functions on Spark DataFrames to manipulate, analyze, and transform your data. Here’s an overview of the kinds of functions available:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Basic DataFrame Operations**\n",
    "\n",
    "* **`select()`** — Select columns:\n",
    "\n",
    "  ```python\n",
    "  df.select(\"col1\", \"col2\")\n",
    "  ```\n",
    "* **`filter()` / `where()`** — Filter rows by condition:\n",
    "\n",
    "  ```python\n",
    "  df.filter(df.col(\"age\") > 30)\n",
    "  ```\n",
    "* **`groupBy()` + aggregation** — Group rows and aggregate:\n",
    "\n",
    "  ```python\n",
    "  df.groupBy(\"category\").count()\n",
    "  df.groupBy(\"category\").agg({\"sales\": \"sum\"})\n",
    "  ```\n",
    "* **`orderBy()` / `sort()`** — Sort rows by columns:\n",
    "\n",
    "  ```python\n",
    "  df.orderBy(\"age\", ascending=False)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Column Functions (from `pyspark.sql.functions`)**\n",
    "\n",
    "You can import many built-in functions to apply on columns, for example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, lit, concat, upper, lower, when, isnan, count\n",
    "\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"upper_name\"),\n",
    "    when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\").alias(\"age_group\")\n",
    ")\n",
    "```\n",
    "\n",
    "Common functions include:\n",
    "\n",
    "* String functions: `upper()`, `lower()`, `concat()`, `substring()`, `trim()`\n",
    "* Math functions: `round()`, `sqrt()`, `abs()`\n",
    "* Conditional functions: `when()`, `otherwise()`\n",
    "* Null handling: `isnull()`, `isnan()`, `coalesce()`\n",
    "* Date/time: `current_date()`, `datediff()`, `year()`, `month()`\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **SQL Expressions**\n",
    "\n",
    "You can run SQL queries on DataFrames registered as temp views:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "spark.sql(\"SELECT name, age FROM my_table WHERE age > 30\").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Joins**\n",
    "\n",
    "Joining DataFrames on keys:\n",
    "\n",
    "```python\n",
    "df1.join(df2, df1.id == df2.id, \"inner\")\n",
    "```\n",
    "\n",
    "Types: inner, left\\_outer, right\\_outer, full\\_outer, cross, semi, anti\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Window Functions**\n",
    "\n",
    "Perform operations over a sliding window of rows:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"sales\")\n",
    "df.withColumn(\"rank\", row_number().over(window_spec)).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Aggregations**\n",
    "\n",
    "* `agg()`, `sum()`, `avg()`, `min()`, `max()`, `count()`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"category\").agg({\"sales\": \"sum\", \"price\": \"avg\"}).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **User-Defined Functions (UDFs)**\n",
    "\n",
    "Write your own functions for custom transformations:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def plus_one(x):\n",
    "    return str(int(x) + 1)\n",
    "\n",
    "plus_one_udf = udf(plus_one, StringType())\n",
    "df.withColumn(\"new_col\", plus_one_udf(df[\"age\"])).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Other Useful Functions**\n",
    "\n",
    "* `withColumn()` — Add or replace a column\n",
    "* `drop()` — Drop columns\n",
    "* `distinct()` — Remove duplicates\n",
    "* `limit()` — Take first N rows\n",
    "* `cache()` / `persist()` — Cache DataFrame in memory/disk\n",
    "* `explain()` — Show query plan for optimization insight\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Function Type          | Examples                            | Purpose                             |\n",
    "| ---------------------- | ----------------------------------- | ----------------------------------- |\n",
    "| Basic ops              | `select()`, `filter()`, `groupBy()` | Data selection, filtering, grouping |\n",
    "| Column functions       | `upper()`, `when()`, `concat()`     | Column transformations              |\n",
    "| SQL                    | `spark.sql(\"SELECT ...\")`           | SQL querying                        |\n",
    "| Joins                  | `join()`                            | Combining data                      |\n",
    "| Window functions       | `row_number()`, `rank()`            | Analytics over partitions           |\n",
    "| Aggregations           | `sum()`, `count()`, `avg()`         | Summary statistics                  |\n",
    "| User Defined Functions | `udf()`                             | Custom transformations              |\n",
    "| DataFrame management   | `withColumn()`, `drop()`, `cache()` | Modify columns, optimize caching    |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can give examples of any of these or show how to chain multiple functions together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38780493-f8c0-4ae9-b244-a4e36ad6e838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
