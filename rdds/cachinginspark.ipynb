{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a14312a8-291d-49fc-b49a-fba252e26b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfec8f0b-07c6-48e1-b04c-465403f9a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalone\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a8452ad-0499-45c4-a5ec-ad75d3ef29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5cdfd92-19f1-4e84-8da5-c11340ff3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you havent cached anything then it starts from beginning ie reading files, partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1d449df-2122-41a9-85c9-dadf6330b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize([i for i in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "733da0c3-bea8-4608-972f-989a60e8e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.map(lambda x: x+1)\n",
    "rdd3=rdd2.map(lambda x: x+1)\n",
    "rdd4=rdd3.map(lambda x: x+1)\n",
    "rdd5=rdd4.map(lambda x: x+1)\n",
    "rdd6=rdd5.map(lambda x: x+1)\n",
    "rdd7=rdd6.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "813c6ead-d36e-4a10-8866-45482a801c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "x=rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e28350bb-32c3-4423-b041-df9b227f045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "x=rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b84c9ee6-967a-4cb6-bc5f-0d425fdc492e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[1] at collect at /tmp/ipykernel_770/584931559.py:1 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:297 []'\n"
     ]
    }
   ],
   "source": [
    "print(rdd7.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043c462-b4e4-4eac-a9b1-db80bc3784c0",
   "metadata": {},
   "source": [
    "# vanna khoje ko when we do rdd7 .collect 2nd time as well it went back to create partions\n",
    "so rdd7 back janxa janxa cached rdd vetyo vanae tei use garxa\n",
    "\n",
    "if cached vete na vanae feri partition banauxa\n",
    "\n",
    "rdd 7 sanga jo jo linked xa tyo matrai rdd calculate hunxa paxadi jada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d5aa8-b1d8-448f-a704-83382c7c2bd2",
   "metadata": {},
   "source": [
    "# caching/persist concepts\n",
    "\n",
    "tyo calculate vako partition memory mai store gari rakhu pare so that it doesnot go back every time to create partitions from the start\n",
    "\n",
    "tyo partiitions haru store garnu paryo vanae cache persisit use garni\n",
    "\n",
    "persist mempry only vanae ko nai cache ho\n",
    "\n",
    "yedi memory only xa aani memory full xa vanae LRU(least recenty used) algorithm use hunxa to remove partitions and free space for the partitions to cache\n",
    "\n",
    "\n",
    "we can do memory and disk ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f5c0936-e0a0-4789-8b39-a14a70710bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.map(lambda x: x+1)\n",
    "rdd3=rdd2.map(lambda x: x+1)\n",
    "rdd4=rdd3.map(lambda x: x+1).cache()\n",
    "rdd5=rdd4.map(lambda x: x+1)\n",
    "rdd6=rdd5.map(lambda x: x+1)\n",
    "rdd7=rdd6.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bfd2a32-9b45-46cb-a7ac-e0d97028789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b50d7985-bde8-4fb1-87cc-ad423dbe159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=rdd7.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea5ece-bb0c-4eb7-9fbd-b98eb96172c9",
   "metadata": {},
   "source": [
    "In the **2nd run**, if you properly cached `rdd4` and the cache is still available, **Spark will use the cached data for `rdd4`** instead of recomputing from `rdd1`.\n",
    "\n",
    "---\n",
    "\n",
    "### How it works:\n",
    "\n",
    "* On the **1st run**, since `rdd4` is not yet computed, Spark starts from `rdd1` and runs the full chain up to `rdd4`, caches it, then continues to `rdd7`.\n",
    "* On the **2nd run**, Spark **checks the cache** for `rdd4`:\n",
    "\n",
    "  * If the cached data exists and is not evicted, Spark **loads `rdd4` from cache**, skipping all upstream transformations from `rdd1` to `rdd3`.\n",
    "  * Then it continues transformations from `rdd4` to `rdd7`.\n",
    "\n",
    "---\n",
    "\n",
    "### Important:\n",
    "\n",
    "* The cache lives **only as long as your Spark context is alive and memory/disk space is sufficient**.\n",
    "* If the cached data is evicted (e.g., due to memory pressure), Spark will recompute it again.\n",
    "* If you call `.unpersist()` on `rdd4`, the cache is removed and Spark recomputes next time.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR:\n",
    "\n",
    "| Run     | Behavior                                        |\n",
    "| ------- | ----------------------------------------------- |\n",
    "| 1st run | Computes from `rdd1` → caches `rdd4` → finishes |\n",
    "| 2nd run | Loads cached `rdd4` → computes rest             |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can help you write a quick test to verify this caching behavior!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d52a3-c153-441f-8522-91804d3de23c",
   "metadata": {},
   "source": [
    "rdd4 ma action gare ko vayae bich tirai obviously tei cached nai use garxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7de7b17-6a43-4e76-b976-8b87e9608e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "485e4b26-8637-4626-b6f1-b42b28f7c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e075c47-7bfc-4ff4-b6bf-e41472ebf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3=rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "765ec774-e596-4604-9ac9-95cf637c157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z4=rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd374a-808a-48b6-ae15-7d1308450ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
