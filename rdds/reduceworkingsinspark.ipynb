{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdd942d-a0cc-4722-ac16-0b57dc360e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/23 12:17:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterStandalone\") \\\n",
    "    .master(\"spark://8fa087ac675c:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "847b2052-420f-442a-9011-470d1004f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize([(\"key1\",1),(\"key1\",9),(\"key1\",43),(\"key1\",54),(\"key1\",76),(\"key1\",75),(\"key1\",31),(\"key1\",85),(\"key1\",97),(\"key1\",57),(\"key1\",25),(\"key1\",94),(\"key1\",14)],3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c31a347-6fbb-44e9-b468-b536ea3112ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.saveAsTextFile(\"/home/original2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2e0a7ba-0d0b-44dd-b4ff-39fe1ab7f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducerfunc(x,y):\n",
    "    with open(\"/home/m3valuesofxandy.txt\",\"a\") as file:\n",
    "        file.write(f\"value of x={x} and value of y={y}\")\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "939fa485-12e6-4e7c-b0c4-21792b9b7eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('key1', 1), ('key1', 9), ('key1', 43), ('key1', 54)],\n",
       " [('key1', 76), ('key1', 75), ('key1', 31), ('key1', 85)],\n",
       " [('key1', 97), ('key1', 57), ('key1', 25), ('key1', 94), ('key1', 14)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2db4ed-f87c-4a22-819b-13d4a1881e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.reduceByKey(reducerfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92905f0e-133c-4b94-929e-a28296cc24a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 661)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63b995-8e98-4d02-a3d8-380a8894c695",
   "metadata": {},
   "source": [
    "# output for executor /worker no 1\n",
    "\n",
    "value of x=97 and value of y=57value of x=154 and value of y=25value of x=179 and value of y=94value of x=273 and value of y=14value of x=1 and value of y=9value of x=10 and value of y=43value of x=53 and value of y=54value of x=287 and value of y=107value of x=394 and value of y=267\n",
    "\n",
    "\n",
    "we can see that partition 1 and 3 lied in worker 1 \n",
    "\n",
    "it did sum as \n",
    "\n",
    "x=sumupto this but if it is first sum then x= any of the data\n",
    "\n",
    "y= another data\n",
    "\n",
    "it calculated sum for all partitions\n",
    "\n",
    "\n",
    "so local sum\n",
    "\n",
    "then it is shuffled across partitions so that values with same key end in same executor\n",
    "\n",
    "now in the executor where the key has ended up \n",
    "\n",
    "as we can see in this case also the sum of key1 key has ended up in this cluster as well\n",
    "\n",
    "hamlae yo verify garna sakxam mathi ko data bata\n",
    "\n",
    "\n",
    "as partition ko sum 267 pani yei worker node ma aai raa xa\n",
    "\n",
    "so yellae 3 nai oota ko sum garxa \n",
    "\n",
    "aani result dini vayo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8877c2c-aa3a-454e-852c-751847f8f515",
   "metadata": {},
   "source": [
    "Absolutely! Let's unpack **what `x` and `y` really are** in the lambda function used in `reduceByKey`.\n",
    "\n",
    "---\n",
    "\n",
    "### When you do:\n",
    "\n",
    "```python\n",
    "rdd.reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What are `x` and `y`?\n",
    "\n",
    "* The **RDD is a collection of key-value pairs**, like:\n",
    "\n",
    "  ```python\n",
    "  [('a', 1), ('a', 2), ('b', 5), ('a', 3)]\n",
    "  ```\n",
    "\n",
    "* For each **key**, Spark wants to combine all the values.\n",
    "\n",
    "* `reduceByKey` applies the lambda function to **pairs of values for the same key**, repeatedly, until all values for that key are combined into one.\n",
    "\n",
    "---\n",
    "\n",
    "### So,\n",
    "\n",
    "* **`x` and `y` are values associated with the same key.**\n",
    "\n",
    "* For key `'a'`, Spark will do something like:\n",
    "\n",
    "  1. Take first two values: `x = 1`, `y = 2` → combine (`1 + 2 = 3`)\n",
    "  2. Take result (`3`) and next value `3`: `x = 3`, `y = 3` → combine (`3 + 3 = 6`)\n",
    "\n",
    "* The function (`lambda x, y: x + y`) tells how to combine two values at a time.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual:\n",
    "\n",
    "| Step | x | y | Result of x + y |\n",
    "| ---- | - | - | --------------- |\n",
    "| 1    | 1 | 2 | 3               |\n",
    "| 2    | 3 | 3 | 6               |\n",
    "\n",
    "---\n",
    "\n",
    "### Important points:\n",
    "\n",
    "* You can replace `lambda x, y: x + y` with any **binary function** that combines two values, like max, min, multiplication, etc.\n",
    "\n",
    "* Spark calls this function **many times internally** to merge all values for each key.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR:\n",
    "\n",
    "* `x` and `y` are **two values from the same key’s value list**.\n",
    "* The function defines how to merge those two values.\n",
    "* Spark does this repeatedly until **all values for the key are merged into one**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can write a small example with print statements to show how Spark calls this function internally! Would you like that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d337caa-a75e-45fd-99c9-78ed241f5bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
